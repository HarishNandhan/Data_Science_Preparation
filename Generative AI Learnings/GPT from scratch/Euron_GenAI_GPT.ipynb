{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AVvlhBFqUKS",
        "outputId": "599f0278-d9e9-4c77-b8c8-7829abc849d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: tensorflow in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.9.2)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yoges\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "H6vhGaHWw0IS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "3Ihe9TFUw2wt"
      },
      "outputs": [],
      "source": [
        "with open(r\"D:\\Resources\\GenAI\\GPT from scratch\\data.txt\",'r',encoding='utf-8') as f:\n",
        "  text_data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "gjY7yCVixCRh",
        "outputId": "7f3d9f5e-cefb-42cb-d1f3-1a164fd8dcdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"my name is sudhanshu kumar , i work with euron , Helping Millions of Students Succeed\\nSudhanshu's commitment to affordable education wasn't just a business strategy—it was his life's mission. Over the years, iNeuron has helped over 1.5 million students from 34+ countries, providing them with the skills they need to succeed in today's competitive job market. Many of these students, like Sudhanshu himself, came from disadvantaged backgrounds. They saw iNeuron as a lifeline—an opportunity to rise above their circumstances.\\n\\nIn 2022, iNeuron was acquired by PhysicsWallah in a deal worth ₹250 crore. While this acquisition was a significant milestone, Sudhanshu remained focused on his mission. Even after the acquisition, iNeuron continued to offer some of the most affordable and accessible tech courses in the world.\\nHelping Millions of Students Succeed\\nSudhanshu's commitment to affordable education wasn't just a business strategy—it was his life's mission. Over the years, iNeuron has helped over 1.5 million students from 34+ countries, providing them with the skills they need to succeed in today's competitive job market. Many of these students, like Sudhanshu himself, came from disadvantaged backgrounds. They saw iNeuron as a lifeline—an opportunity to rise above their circumstances.\\n\\nIn 2022, iNeuron was acquired by PhysicsWallah in a deal worth ₹250 crore. While this acquisition was a significant milestone, Sudhanshu remained focused on his mission. Even after the acquisition, iNeuron continued to offer some of the most affordable and accessible tech courses in the world.Sudhanshu Kumar's life is a story of triumph over adversity, driven by the belief in the transformative power of education. Born in Jamshedpur, Jharkhand, India, to a family of very modest means, Sudhanshu's early years were marked by financial hardship. His surroundings offered little opportunity, and resources were limited, yet he understood from a young age that education could be his ticket out of poverty.\\n\\nWhile many would have been daunted by the lack of support and opportunity, Sudhanshu was relentless in his pursuit of knowledge. He knew that education had the power to change lives, and he was determined to leverage it to create a better future for himself and his family. Despite the numerous challenges along the way, Sudhanshu excelled academically, eventually earning a degree in Computer Science and Engineering (CSE).\\nAfter completing his education, Sudhanshu began his professional journey in the tech industry, working with prestigious companies like Wipro, Deloitte, Verizon Labs, and Ernst & Young. During this time, he gained expertise in various technologies and frameworks, including SAP WebDynpro, Fiori UI5 HANA, Java, Big Data, Data Analytics, and more. He became a well-rounded technologist, well-respected in his field.\\n\\nDespite his growing success, Sudhanshu never forgot his roots. He knew that there were many others who, like him, came from humble backgrounds and were looking for an opportunity to change their lives through education. It was during this time that Sudhanshu realized a harsh truth: quality education was often inaccessible to those who needed it the most. The high cost of education barred millions of people from pursuing their dreams, especially in tech fields that required specialized skills.\\nFueled by his passion for making education accessible, Sudhanshu decided to take action. In 2019, he founded iNeuron Intelligence Private Limited, an edtech platform that would make tech upskilling affordable and accessible for everyone. His mission was clear: to provide high-quality courses at a price so low that even those from the most disadvantaged backgrounds could afford to learn.\\n\\niNeuron was designed to be more than just an online learning platform. It offered a comprehensive bundle of resources, including courses, books, hands-on projects, and live classes, making sure that learners could gain real-world, applicable skills. Most importantly, iNeuron was priced to ensure no student would be left behind due to financial constraints.\\n\\nThe company quickly gained traction, thanks to Sudhanshu's focus on affordability and quality. In 2021, iNeuron raised $3 million in funding from S. Chand, a leading education publisher. This allowed iNeuron to expand its offerings and reach a larger audience.\\nBuilding on the success of iNeuron, Sudhanshu is now leading Euron, a unified platform for tech upskilling. Euron is designed for enterprises, schools, colleges, government organizations, and individuals looking to improve their tech skills. The platform provides a comprehensive bundle of resources, including courses, books, live classes, and projects. Euron's unique offering is its ability to provide licenses at scale—organizations can subscribe to provide their teams with unlimited access to learning materials for just 1600 INR per year per license.\\n\\nFor individuals, Euron Plus offers a similar plan, priced at 2900 INR per year, giving learners access to all of Euron's content, along with 24/7 support through Euron Assist. The idea is simple: anyone, anywhere, should have the opportunity to upskill without financial barriers.\\n\\nThe Euron Motto: Education for All, Without Limits\\n\\nAt the heart of Euron's philosophy is a single, powerful idea: Education for All, Without Limits. Sudhanshu believes that every person deserves the chance to learn, grow, and succeed, regardless of their financial situation or background. This belief is what drives Euron's mission to make tech education not just affordable but also accessible to anyone, anywhere in the world.\""
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "yp4mc6FYxCyh"
      },
      "outputs": [],
      "source": [
        "# creating embeddings\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000,oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQEqLvuFx6-e",
        "outputId": "28ff2bad-77f9-4c76-f6e5-57355f23b821"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.preprocessing.text.Tokenizer at 0x2701b2ec1c0>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "6vDDuQzzx8FZ"
      },
      "outputs": [],
      "source": [
        "sequence = tokenizer.texts_to_sequences([text_data])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_lKd7-ByBbt",
        "outputId": "653fc762-e501-46b2-a5c0-77ba7c98ee49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "860"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "38Ci9RHjyKoh"
      },
      "outputs": [],
      "source": [
        "with open(\"tokenizer.pkl\",'wb') as f:\n",
        "  pickle.dump(tokenizer,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TkQvFf7t2Rdf"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 100\n",
        "def create_dataset(seq,window_size = max_seq_length):\n",
        "    input ,labels = [],[]\n",
        "    for i in range(len(seq) -window_size):\n",
        "        input.append(seq[i:i+window_size])\n",
        "        labels.append(seq[i+1:i+window_size+1])\n",
        "    return np.array(input) , np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "_whg84bG7Y8W"
      },
      "outputs": [],
      "source": [
        "x_data, y_data = create_dataset(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAN53GzT7eX2",
        "outputId": "15f935c3-8650-4b03-9253-9ac960b98d2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "760"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(x_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N5JGfX57u4W",
        "outputId": "dd39b37b-1008-4096-9a90-d05f836c3f66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([158, 159,  16,   9, 160, 161, 162,  20,  21,  69,  49,   5,  22,\n",
              "        28,  36,  70,   2,  23,  10,  71,  29,   4,  72,  73,  12,   8,\n",
              "        74,  24,  30,   3,  50,  11,  75,  76,  30,  77,  78,  51,  22,\n",
              "        14,  79,  80,  81,  82,  20,   3,  31,  37,  83,   2,  28,   6,\n",
              "        84,  85,  86,  87,  38,   5,  88,  22,  39,   9,  52,  53,  14,\n",
              "        54,  40,  37,  89,  11,  90,   4,  91,  25,   2,  92,  93,  18,\n",
              "        94,   6,  95,  11,  12,  96,  26,  97,   6,   4,  98,  99, 100,\n",
              "       101,  55,  27,  41,  12,   4, 102, 103,   9])"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# input\n",
        "x_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQjsF4Eo7v_G",
        "outputId": "474d5aae-ae63-47b7-f976-6498d240eef3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([159,  16,   9, 160, 161, 162,  20,  21,  69,  49,   5,  22,  28,\n",
              "        36,  70,   2,  23,  10,  71,  29,   4,  72,  73,  12,   8,  74,\n",
              "        24,  30,   3,  50,  11,  75,  76,  30,  77,  78,  51,  22,  14,\n",
              "        79,  80,  81,  82,  20,   3,  31,  37,  83,   2,  28,   6,  84,\n",
              "        85,  86,  87,  38,   5,  88,  22,  39,   9,  52,  53,  14,  54,\n",
              "        40,  37,  89,  11,  90,   4,  91,  25,   2,  92,  93,  18,  94,\n",
              "         6,  95,  11,  12,  96,  26,  97,   6,   4,  98,  99, 100, 101,\n",
              "        55,  27,  41,  12,   4, 102, 103,   9, 104])"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is the output which the input tries to learn\n",
        "y_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "RVU05_0r7xdW"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_len, d_model, **kwargs):  # <-- add **kwargs\n",
        "        super().__init__(**kwargs)                   # <-- pass kwargs here too\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "        pos = np.arange(max_len)[:, np.newaxis]\n",
        "        i = np.arange(d_model)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        angle_rads = pos * angle_rates\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"max_len\": self.max_len,\n",
        "            \"d_model\": self.d_model\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ieTdzCCh_obU"
      },
      "outputs": [],
      "source": [
        "def transformer_block(embed_dim, num_heads, ff_dim, dropout = 0.1):\n",
        "\n",
        "  # input layer\n",
        "  inputs = layers.Input(shape = (None,embed_dim))\n",
        "  # Multi head attention\n",
        "  attn_output = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)(inputs,inputs)\n",
        "  # Dropout\n",
        "  attn_output = layers.Dropout(dropout)(attn_output)\n",
        "  # Layer Normalization\n",
        "  out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "\n",
        "  # Feed Forward Neural network\n",
        "  ffn = tf.keras.Sequential([\n",
        "   layers.Dense(ff_dim,activation='relu'),\n",
        "   layers.Dense(embed_dim)\n",
        "  ])\n",
        "\n",
        "  ffn_output = ffn(out1)\n",
        "  ffn_output = layers.Dropout(dropout)(ffn_output)\n",
        "  out2 = layers.LayerNormalization(epsilon=1e-6)(out1+ffn_output)\n",
        "  return tf.keras.Model(inputs = inputs, outputs = out2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "mg7O9QZuByGG"
      },
      "outputs": [],
      "source": [
        "vocab_size = 5000\n",
        "max_seq_len = 100\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "ff_dim = 512\n",
        "num_layers = 4\n",
        "batch_size = 8\n",
        "epoch  = 25\n",
        "\n",
        "def build_gpt_model():\n",
        "\n",
        "  # input\n",
        "  inputs = layers.Input(shape=(max_seq_len,))\n",
        "  x = layers.Embedding(input_dim = vocab_size, output_dim = embed_dim)(inputs)\n",
        "  x = PositionalEncoding(max_seq_len,embed_dim)(x)\n",
        "\n",
        "  # stack of transformer\n",
        "  for _ in range(num_layers):\n",
        "    x = transformer_block(embed_dim=embed_dim,num_heads=num_heads, ff_dim=ff_dim)(x)\n",
        "\n",
        "  outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "  return tf.keras.Model(inputs,outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "UuYf2UyPJCMn"
      },
      "outputs": [],
      "source": [
        "model = build_gpt_model()\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YudJ8WGIJd9f",
        "outputId": "c38f5501-68ea-4f0a-f680-40fa9e914a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "86/86 [==============================] - 10s 41ms/step - loss: 5.7591 - accuracy: 0.0304 - val_loss: 7.3054 - val_accuracy: 0.0468\n",
            "Epoch 2/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3388 - accuracy: 0.0325 - val_loss: 7.3037 - val_accuracy: 0.0468\n",
            "Epoch 3/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3381 - accuracy: 0.0326 - val_loss: 7.5771 - val_accuracy: 0.0468\n",
            "Epoch 4/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3374 - accuracy: 0.0328 - val_loss: 7.4269 - val_accuracy: 0.0437\n",
            "Epoch 5/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3811 - accuracy: 0.0329 - val_loss: 7.5479 - val_accuracy: 0.0468\n",
            "Epoch 6/25\n",
            "86/86 [==============================] - 3s 32ms/step - loss: 5.3333 - accuracy: 0.0327 - val_loss: 7.3975 - val_accuracy: 0.0468\n",
            "Epoch 7/25\n",
            "86/86 [==============================] - 3s 32ms/step - loss: 5.3329 - accuracy: 0.0331 - val_loss: 7.5705 - val_accuracy: 0.0468\n",
            "Epoch 8/25\n",
            "86/86 [==============================] - 3s 32ms/step - loss: 5.3243 - accuracy: 0.0330 - val_loss: 7.3575 - val_accuracy: 0.0468\n",
            "Epoch 9/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3311 - accuracy: 0.0332 - val_loss: 7.7365 - val_accuracy: 0.0437\n",
            "Epoch 10/25\n",
            "86/86 [==============================] - 3s 32ms/step - loss: 5.3311 - accuracy: 0.0336 - val_loss: 7.5220 - val_accuracy: 0.0468\n",
            "Epoch 11/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3311 - accuracy: 0.0319 - val_loss: 7.6363 - val_accuracy: 0.0437\n",
            "Epoch 12/25\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 5.3319 - accuracy: 0.0328 - val_loss: 7.6241 - val_accuracy: 0.0468\n",
            "Epoch 13/25\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 5.3232 - accuracy: 0.0334 - val_loss: 7.3690 - val_accuracy: 0.0468\n",
            "Epoch 14/25\n",
            "86/86 [==============================] - 3s 36ms/step - loss: 5.3311 - accuracy: 0.0322 - val_loss: 7.6154 - val_accuracy: 0.0468\n",
            "Epoch 15/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3279 - accuracy: 0.0329 - val_loss: 7.5868 - val_accuracy: 0.0187\n",
            "Epoch 16/25\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 5.3274 - accuracy: 0.0335 - val_loss: 7.6386 - val_accuracy: 0.0468\n",
            "Epoch 17/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3317 - accuracy: 0.0334 - val_loss: 7.4928 - val_accuracy: 0.0437\n",
            "Epoch 18/25\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 5.3313 - accuracy: 0.0321 - val_loss: 7.5665 - val_accuracy: 0.0468\n",
            "Epoch 19/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3256 - accuracy: 0.0334 - val_loss: 7.5997 - val_accuracy: 0.0468\n",
            "Epoch 20/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3247 - accuracy: 0.0327 - val_loss: 7.5037 - val_accuracy: 0.0437\n",
            "Epoch 21/25\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 5.3250 - accuracy: 0.0326 - val_loss: 7.6960 - val_accuracy: 0.0187\n",
            "Epoch 22/25\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 5.3235 - accuracy: 0.0331 - val_loss: 7.5813 - val_accuracy: 0.0468\n",
            "Epoch 23/25\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 5.3226 - accuracy: 0.0340 - val_loss: 7.4878 - val_accuracy: 0.0468\n",
            "Epoch 24/25\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 5.3232 - accuracy: 0.0330 - val_loss: 7.6342 - val_accuracy: 0.0468\n",
            "Epoch 25/25\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 5.3246 - accuracy: 0.0332 - val_loss: 7.6059 - val_accuracy: 0.0468\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x27020c57490>"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_data,y_data,batch_size=batch_size,epochs=epoch,validation_split=0.10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "oJSnP4cSN2T2"
      },
      "outputs": [],
      "source": [
        "model.save('gpt_test_genai_class.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "em0rES_YPlhA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_121\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_118 (InputLayer)      [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, 100, 128)          640000    \n",
            "                                                                 \n",
            " positional_encoding_5 (Posi  (None, 100, 128)         0         \n",
            " tionalEncoding)                                                 \n",
            "                                                                 \n",
            " model_117 (Functional)      (None, None, 128)         396032    \n",
            "                                                                 \n",
            " model_118 (Functional)      (None, None, 128)         396032    \n",
            "                                                                 \n",
            " model_119 (Functional)      (None, None, 128)         396032    \n",
            "                                                                 \n",
            " model_120 (Functional)      (None, None, 128)         396032    \n",
            "                                                                 \n",
            " dense_237 (Dense)           (None, 100, 5000)         645000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,869,128\n",
            "Trainable params: 2,869,128\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load trained model\n",
        "model = tf.keras.models.load_model(\"gpt_test_genai_class.h5\", custom_objects={\"PositionalEncoding\": PositionalEncoding})\n",
        "\n",
        "# Load tokenizer\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# Parameters\n",
        "max_seq_len = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(seed_text, model, tokenizer, num_tokens=50, temperature=1.0):\n",
        "    for _ in range(num_tokens):\n",
        "        token_seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_seq = token_seq[-max_seq_len:]  # Trim to max length\n",
        "        padded_seq = pad_sequences([token_seq], maxlen=max_seq_len)\n",
        "\n",
        "        preds = model.predict(padded_seq, verbose=0)[0, -1]  # Get prediction for last time step\n",
        "        preds = np.asarray(preds).astype('float64')\n",
        "\n",
        "        # Apply temperature sampling\n",
        "        preds = np.log(preds + 1e-9) / temperature\n",
        "        preds = np.exp(preds) / np.sum(np.exp(preds))\n",
        "\n",
        "        next_token_id = np.random.choice(len(preds), p=preds)\n",
        "        next_word = tokenizer.index_word.get(next_token_id, '')\n",
        "\n",
        "        seed_text += ' ' + next_word\n",
        "\n",
        "        if next_word == '':  # Optional: break if OOV or unknown word\n",
        "            break\n",
        "\n",
        "    return seed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Generated Text:\n",
            "who is sudhanshu in to most their from to and at they is while the the is gain designed to support 5 a during a thanks sudhanshu significant ineuron a the wipro he sudhanshu's and power importantly books reach the truth ineuron bundle in time due engineering learn the offer a very like\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"who is sudhanshu\"\n",
        "generated = generate_text(seed_text, model, tokenizer, num_tokens=50, temperature=1.0)\n",
        "\n",
        "print(\"📝 Generated Text:\")\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
